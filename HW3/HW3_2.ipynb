{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YousefAbua/Intro-To-DL/blob/main/HW3/HW3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3sx5kkuSnWdt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzFT1KnAoQ_4",
        "outputId": "55558bca-15c7-41a0-975f-4d69d899416c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Setup dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encode the text into integers\n",
        "encoded_text = [char_to_int[ch] for ch in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mxhYxGGQoaUy"
      },
      "outputs": [],
      "source": [
        "def Define_Dataset(max_length):\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in range(len(text) - max_length):\n",
        "    sequence = text[i:i + max_length]\n",
        "    label = text[i + max_length]\n",
        "    x.append([char_to_int[char] for char in sequence])\n",
        "    y.append(char_to_int[label])\n",
        "\n",
        "  x = np.array(x)\n",
        "  y = np.array(y)\n",
        "  return x, y\n",
        "\n",
        "x20, y20 = Define_Dataset(20) # Max Length = 20\n",
        "x30, y30 = Define_Dataset(30) # Max Length = 30\n",
        "x50, y50 = Define_Dataset(50) # Max Length = 50\n",
        "\n",
        "x20 = torch.tensor(x20, dtype=torch.long)\n",
        "y20 = torch.tensor(y20, dtype=torch.long)\n",
        "\n",
        "x30 = torch.tensor(x30, dtype=torch.long)\n",
        "y30 = torch.tensor(y30, dtype=torch.long)\n",
        "\n",
        "x50 = torch.tensor(x50, dtype=torch.long)\n",
        "y50 = torch.tensor(y50, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SQvmpCWOpYi1"
      },
      "outputs": [],
      "source": [
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index], self.targets[index]\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset_20 = CharDataset(x20, y20)\n",
        "dataset_30 = CharDataset(x30, y30)\n",
        "dataset_50 = CharDataset(x50, y50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yUAor_Hhq4An"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "# Sequence 20\n",
        "train_size20 = int(len(dataset_20) * 0.8)\n",
        "test_size20 = len(dataset_20) - train_size20\n",
        "train20_dataset, test20_dataset = torch.utils.data.random_split(dataset_20, [train_size20, test_size20])\n",
        "\n",
        "train20_loader = DataLoader(train20_dataset, shuffle=True, batch_size=batch_size)\n",
        "test20_loader = DataLoader(test20_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Sequence 30\n",
        "train_size30 = int(len(dataset_30) * 0.8)\n",
        "test_size30 = len(dataset_30) - train_size30\n",
        "train30_dataset, test30_dataset = torch.utils.data.random_split(dataset_30, [train_size30, test_size30])\n",
        "\n",
        "train30_loader = DataLoader(train30_dataset, shuffle=True, batch_size=batch_size)\n",
        "test30_loader = DataLoader(test30_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Sequence 50\n",
        "train_size50 = int(len(dataset_50) * 0.8)\n",
        "test_size50 = len(dataset_50) - train_size50\n",
        "train50_dataset, test50_dataset = torch.utils.data.random_split(dataset_50, [train_size50, test_size50])\n",
        "\n",
        "train50_loader = DataLoader(train50_dataset, shuffle=True, batch_size=batch_size)\n",
        "test50_loader = DataLoader(test50_dataset, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YktFd3fTr5Ct"
      },
      "outputs": [],
      "source": [
        "# Define LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = self.fc(output[:, -1, :])  # Taking the last time step output\n",
        "        return output\n",
        "\n",
        "# Define GRU model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.gru(embedded)\n",
        "        output = self.fc(output[:, -1, :])  # Taking the last time step output\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ribOU3gNsPFd"
      },
      "outputs": [],
      "source": [
        "def training_loop(train, test, model, loss_fn, optimizer, epochs):\n",
        "  model.to(device)  # Move model to GPU\n",
        "  # Train/Validation Loop\n",
        "  train_loss_list = []\n",
        "  val_loss_list = []\n",
        "  val_accuracy_list = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      train_loss = 0.0\n",
        "      val_loss = 0.0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "\n",
        "      # Training\n",
        "      model.train()\n",
        "      for sequences, targets in train:\n",
        "          sequences, targets = sequences.to(device), targets.to(device)  # Move data to GPU\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(sequences)\n",
        "          loss = loss_fn(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss += loss.item() * sequences.size(0)\n",
        "\n",
        "      # Validation\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          for sequences, targets in test:\n",
        "              sequences, targets = sequences.to(device), targets.to(device)  # Move data to GPU\n",
        "              outputs = model(sequences)\n",
        "              loss = loss_fn(outputs, targets)\n",
        "              val_loss += loss.item() * sequences.size(0)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += targets.size(0)\n",
        "              correct += (predicted == targets).sum().item()\n",
        "\n",
        "      train_loss = train_loss / len(train.dataset)\n",
        "      val_loss = val_loss / len(test.dataset)\n",
        "      accuracy = correct / total * 100\n",
        "\n",
        "      train_loss_list.append(train_loss)\n",
        "      val_loss_list.append(val_loss)\n",
        "      val_accuracy_list.append(accuracy)\n",
        "\n",
        "      print(f'Epoch [{epoch + 1}/{epochs}], '\n",
        "            f'Training Loss: {train_loss:.4f}, '\n",
        "            f'Validation Loss: {val_loss:.4f}, '\n",
        "            f'Validation Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Prediction function\n",
        "def predict_next_char(model, sequence_length, char_to_int, int_to_char, test_str):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Convert the test string to integers\n",
        "        test_sequence = [char_to_int[char] for char in test_str]\n",
        "        test_sequence = torch.tensor(test_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Predict the next character\n",
        "        output = model(test_sequence)\n",
        "        _, predicted_index = torch.max(output, 1)\n",
        "        predicted_char = int_to_char[predicted_index.item()]\n",
        "\n",
        "    return predicted_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUKmPIcLu9gr",
        "outputId": "e46e66d5-7238-46bd-ebc4-c6d24f7d3d8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training for sequence size: 20....\n",
            "\n",
            "LSTM Model\n",
            "Epoch [1/10], Training Loss: 1.8371, Validation Loss: 1.6440, Validation Accuracy: 51.00%\n",
            "Epoch [2/10], Training Loss: 1.5813, Validation Loss: 1.5524, Validation Accuracy: 53.11%\n",
            "Epoch [3/10], Training Loss: 1.5142, Validation Loss: 1.5177, Validation Accuracy: 54.15%\n",
            "Epoch [4/10], Training Loss: 1.4763, Validation Loss: 1.4875, Validation Accuracy: 54.86%\n",
            "Epoch [5/10], Training Loss: 1.4510, Validation Loss: 1.4700, Validation Accuracy: 55.19%\n",
            "Epoch [6/10], Training Loss: 1.4325, Validation Loss: 1.4599, Validation Accuracy: 55.64%\n",
            "Epoch [7/10], Training Loss: 1.4183, Validation Loss: 1.4483, Validation Accuracy: 55.85%\n",
            "Epoch [8/10], Training Loss: 1.4065, Validation Loss: 1.4415, Validation Accuracy: 56.21%\n",
            "Epoch [9/10], Training Loss: 1.3966, Validation Loss: 1.4385, Validation Accuracy: 56.24%\n",
            "Epoch [10/10], Training Loss: 1.3889, Validation Loss: 1.4309, Validation Accuracy: 56.43%\n",
            "\n",
            "GRU Model\n",
            "Epoch [1/10], Training Loss: 1.7936, Validation Loss: 1.6228, Validation Accuracy: 51.60%\n",
            "Epoch [2/10], Training Loss: 1.5682, Validation Loss: 1.5504, Validation Accuracy: 53.21%\n",
            "Epoch [3/10], Training Loss: 1.5149, Validation Loss: 1.5211, Validation Accuracy: 54.08%\n",
            "Epoch [4/10], Training Loss: 1.4866, Validation Loss: 1.5019, Validation Accuracy: 54.40%\n",
            "Epoch [5/10], Training Loss: 1.4681, Validation Loss: 1.4863, Validation Accuracy: 55.07%\n",
            "Epoch [6/10], Training Loss: 1.4558, Validation Loss: 1.4791, Validation Accuracy: 55.16%\n",
            "Epoch [7/10], Training Loss: 1.4454, Validation Loss: 1.4743, Validation Accuracy: 55.15%\n",
            "Epoch [8/10], Training Loss: 1.4378, Validation Loss: 1.4698, Validation Accuracy: 55.27%\n",
            "Epoch [9/10], Training Loss: 1.4318, Validation Loss: 1.4669, Validation Accuracy: 55.36%\n",
            "Epoch [10/10], Training Loss: 1.4271, Validation Loss: 1.4659, Validation Accuracy: 55.48%\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 128\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "LSTM20_Model = LSTMModel(len(chars),hidden_size, len(chars))\n",
        "GRU20_Model = GRUModel(len(chars), hidden_size, len(chars))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "LSTM20_optimizer = optim.Adam(LSTM20_Model.parameters(), lr=learning_rate)\n",
        "GRU20_optimizer = optim.Adam(GRU20_Model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"Start training for sequence size: 20....\\n\")\n",
        "print(\"LSTM Model\")\n",
        "training_loop(\n",
        "    train = train20_loader,\n",
        "    test = test20_loader,\n",
        "    model = LSTM20_Model,\n",
        "    loss_fn = criterion,\n",
        "    optimizer = LSTM20_optimizer,\n",
        "    epochs = epochs\n",
        ")\n",
        "print(\"\\nGRU Model\")\n",
        "training_loop(\n",
        "    train = train20_loader,\n",
        "    test = test20_loader,\n",
        "    model = GRU20_Model,\n",
        "    loss_fn = criterion,\n",
        "    optimizer = GRU20_optimizer,\n",
        "    epochs = epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqqvP7lexi3o",
        "outputId": "0dfbdbc3-d0de-4fa5-9ec1-a7adbef4a35f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM: Predicted next character: 't'\n",
            "GRU: Predicted next character: 't'\n"
          ]
        }
      ],
      "source": [
        "# Predicting the next character\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next charac\"\n",
        "predicted_char = predict_next_char(LSTM20_Model, 20, char_to_int, int_to_char, test_str)\n",
        "print(f\"LSTM: Predicted next character: '{predicted_char}'\")\n",
        "\n",
        "# Predicting the next character\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next charac\"\n",
        "predicted_char = predict_next_char(GRU20_Model, 20, char_to_int, int_to_char, test_str)\n",
        "print(f\"GRU: Predicted next character: '{predicted_char}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv5jYPXx0MZP",
        "outputId": "5d45c5ce-b446-49c4-9124-18256285db47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training for sequence size: 30....\n",
            "\n",
            "LSTM Model\n",
            "Epoch [1/10], Training Loss: 1.8265, Validation Loss: 1.6284, Validation Accuracy: 51.48%\n",
            "Epoch [2/10], Training Loss: 1.5677, Validation Loss: 1.5417, Validation Accuracy: 53.63%\n",
            "Epoch [3/10], Training Loss: 1.5014, Validation Loss: 1.5001, Validation Accuracy: 54.54%\n",
            "Epoch [4/10], Training Loss: 1.4636, Validation Loss: 1.4758, Validation Accuracy: 55.06%\n",
            "Epoch [5/10], Training Loss: 1.4390, Validation Loss: 1.4609, Validation Accuracy: 55.50%\n",
            "Epoch [6/10], Training Loss: 1.4210, Validation Loss: 1.4458, Validation Accuracy: 56.00%\n",
            "Epoch [7/10], Training Loss: 1.4065, Validation Loss: 1.4366, Validation Accuracy: 56.30%\n",
            "Epoch [8/10], Training Loss: 1.3949, Validation Loss: 1.4305, Validation Accuracy: 56.45%\n",
            "Epoch [9/10], Training Loss: 1.3853, Validation Loss: 1.4224, Validation Accuracy: 56.70%\n",
            "Epoch [10/10], Training Loss: 1.3770, Validation Loss: 1.4173, Validation Accuracy: 56.78%\n",
            "\n",
            "GRU Model\n",
            "Epoch [1/10], Training Loss: 1.7903, Validation Loss: 1.6145, Validation Accuracy: 51.74%\n",
            "Epoch [2/10], Training Loss: 1.5600, Validation Loss: 1.5374, Validation Accuracy: 53.71%\n",
            "Epoch [3/10], Training Loss: 1.5056, Validation Loss: 1.5113, Validation Accuracy: 54.21%\n",
            "Epoch [4/10], Training Loss: 1.4770, Validation Loss: 1.4892, Validation Accuracy: 54.98%\n",
            "Epoch [5/10], Training Loss: 1.4582, Validation Loss: 1.4766, Validation Accuracy: 55.16%\n",
            "Epoch [6/10], Training Loss: 1.4442, Validation Loss: 1.4686, Validation Accuracy: 55.30%\n",
            "Epoch [7/10], Training Loss: 1.4337, Validation Loss: 1.4589, Validation Accuracy: 55.54%\n",
            "Epoch [8/10], Training Loss: 1.4260, Validation Loss: 1.4571, Validation Accuracy: 55.65%\n",
            "Epoch [9/10], Training Loss: 1.4198, Validation Loss: 1.4490, Validation Accuracy: 55.98%\n",
            "Epoch [10/10], Training Loss: 1.4142, Validation Loss: 1.4495, Validation Accuracy: 55.96%\n"
          ]
        }
      ],
      "source": [
        "LSTM30_Model = LSTMModel(len(chars),hidden_size, len(chars))\n",
        "GRU30_Model = GRUModel(len(chars), hidden_size, len(chars))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "LSTM30_optimizer = optim.Adam(LSTM30_Model.parameters(), lr=learning_rate)\n",
        "GRU30_optimizer = optim.Adam(GRU30_Model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"Start training for sequence size: 30....\\n\")\n",
        "print(\"LSTM Model\")\n",
        "training_loop(\n",
        "    train = train30_loader,\n",
        "    test = test30_loader,\n",
        "    model = LSTM30_Model,\n",
        "    loss_fn = criterion,\n",
        "    optimizer = LSTM30_optimizer,\n",
        "    epochs = epochs\n",
        ")\n",
        "print(\"\\nGRU Model\")\n",
        "training_loop(\n",
        "    train = train30_loader,\n",
        "    test = test30_loader,\n",
        "    model = GRU30_Model,\n",
        "    loss_fn = criterion,\n",
        "    optimizer = GRU30_optimizer,\n",
        "    epochs = epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DJBhoefj0bsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2edff86-4050-46b1-b6fd-0e0ab7f78dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM: Predicted next character: 't'\n",
            "GRU: Predicted next character: 't'\n"
          ]
        }
      ],
      "source": [
        "# Predicting the next character\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next charac\"\n",
        "predicted_char = predict_next_char(LSTM30_Model, 30, char_to_int, int_to_char, test_str)\n",
        "print(f\"LSTM: Predicted next character: '{predicted_char}'\")\n",
        "\n",
        "# Predicting the next character\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next charac\"\n",
        "predicted_char = predict_next_char(GRU30_Model, 30, char_to_int, int_to_char, test_str)\n",
        "print(f\"GRU: Predicted next character: '{predicted_char}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pMb--n6m0kp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec2e444-0dd7-4cec-d278-95eaaf865d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training for sequence size: 50....\n",
            "\n",
            "LSTM Model\n",
            "Epoch [1/10], Training Loss: 1.8215, Validation Loss: 1.6215, Validation Accuracy: 51.80%\n",
            "Epoch [2/10], Training Loss: 1.5637, Validation Loss: 1.5355, Validation Accuracy: 53.78%\n",
            "Epoch [3/10], Training Loss: 1.4963, Validation Loss: 1.4896, Validation Accuracy: 55.18%\n",
            "Epoch [4/10], Training Loss: 1.4588, Validation Loss: 1.4663, Validation Accuracy: 55.73%\n",
            "Epoch [5/10], Training Loss: 1.4332, Validation Loss: 1.4462, Validation Accuracy: 56.15%\n",
            "Epoch [6/10], Training Loss: 1.4139, Validation Loss: 1.4328, Validation Accuracy: 56.49%\n",
            "Epoch [7/10], Training Loss: 1.3995, Validation Loss: 1.4233, Validation Accuracy: 56.81%\n",
            "Epoch [8/10], Training Loss: 1.3875, Validation Loss: 1.4181, Validation Accuracy: 56.86%\n",
            "Epoch [9/10], Training Loss: 1.3778, Validation Loss: 1.4103, Validation Accuracy: 57.02%\n",
            "Epoch [10/10], Training Loss: 1.3690, Validation Loss: 1.4042, Validation Accuracy: 57.33%\n",
            "\n",
            "GRU Model\n",
            "Epoch [1/10], Training Loss: 1.7825, Validation Loss: 1.5973, Validation Accuracy: 52.40%\n",
            "Epoch [2/10], Training Loss: 1.5482, Validation Loss: 1.5232, Validation Accuracy: 54.26%\n",
            "Epoch [3/10], Training Loss: 1.4950, Validation Loss: 1.4954, Validation Accuracy: 54.86%\n",
            "Epoch [4/10], Training Loss: 1.4666, Validation Loss: 1.4718, Validation Accuracy: 55.70%\n",
            "Epoch [5/10], Training Loss: 1.4480, Validation Loss: 1.4617, Validation Accuracy: 55.84%\n",
            "Epoch [6/10], Training Loss: 1.4354, Validation Loss: 1.4543, Validation Accuracy: 56.04%\n",
            "Epoch [7/10], Training Loss: 1.4262, Validation Loss: 1.4478, Validation Accuracy: 56.21%\n",
            "Epoch [8/10], Training Loss: 1.4181, Validation Loss: 1.4430, Validation Accuracy: 56.45%\n",
            "Epoch [9/10], Training Loss: 1.4120, Validation Loss: 1.4434, Validation Accuracy: 56.41%\n",
            "Epoch [10/10], Training Loss: 1.4070, Validation Loss: 1.4348, Validation Accuracy: 56.52%\n"
          ]
        }
      ],
      "source": [
        "LSTM50_Model = LSTMModel(len(chars),hidden_size, len(chars))\n",
        "GRU50_Model = GRUModel(len(chars), hidden_size, len(chars))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "LSTM50_optimizer = optim.Adam(LSTM50_Model.parameters(), lr=learning_rate)\n",
        "GRU50_optimizer = optim.Adam(GRU50_Model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"Start training for sequence size: 50....\\n\")\n",
        "print(\"LSTM Model\")\n",
        "training_loop(\n",
        "    train = train50_loader,\n",
        "    test = test50_loader,\n",
        "    model = LSTM50_Model,\n",
        "    loss_fn = criterion,\n",
        "    optimizer = LSTM50_optimizer,\n",
        "    epochs = epochs\n",
        ")\n",
        "print(\"\\nGRU Model\")\n",
        "training_loop(\n",
        "    train = train50_loader,\n",
        "    test = test50_loader,\n",
        "    model = GRU50_Model,\n",
        "    loss_fn = criterion,\n",
        "    optimizer = GRU50_optimizer,\n",
        "    epochs = epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the next character\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next charac\"\n",
        "predicted_char = predict_next_char(LSTM50_Model, 50, char_to_int, int_to_char, test_str)\n",
        "print(f\"LSTM: Predicted next character: '{predicted_char}'\")\n",
        "\n",
        "# Predicting the next character\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next charac\"\n",
        "predicted_char = predict_next_char(GRU50_Model, 50, char_to_int, int_to_char, test_str)\n",
        "print(f\"GRU: Predicted next character: '{predicted_char}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKIC6b2DCSD_",
        "outputId": "6137a6f6-c235-4c3e-8726-f86f5804197d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM: Predicted next character: 'h'\n",
            "GRU: Predicted next character: 't'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7v5HjslCVcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}