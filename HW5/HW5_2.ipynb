{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YousefAbua/Intro-To-DL/blob/main/HW5/HW5_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HwlAqTsQqLpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e5db80-af02-48b8-a1a6-a40f1d6178f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encode the text into integers\n",
        "encoded_text = [char_to_int[ch] for ch in text]\n",
        "\n",
        "def Define_Dataset(max_length):\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in range(len(text) - max_length):\n",
        "    sequence = text[i:i + max_length]\n",
        "    label = text[i + max_length]\n",
        "    x.append([char_to_int[char] for char in sequence])\n",
        "    y.append(char_to_int[label])\n",
        "\n",
        "  x = np.array(x)\n",
        "  y = np.array(y)\n",
        "  return x, y\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index], self.targets[index]\n",
        "\n",
        "batch_size = 128\n",
        "#sequence_length = 20\n",
        "#sequence_length = 30\n",
        "sequence_length = 50\n",
        "\n",
        "x, y = Define_Dataset(sequence_length)\n",
        "x = torch.tensor(x, dtype=torch.long)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "dataset = CharDataset(x, y)\n",
        "train_size = int(len(dataset) * 0.8)\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "pBjrWv6HVH0c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(train, test, model, loss_fn, optimizer, epochs):\n",
        "  model.to(device)  # Move model to GPU\n",
        "  # Train/Validation Loop\n",
        "  train_loss_list = []\n",
        "  val_loss_list = []\n",
        "  val_accuracy_list = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      train_loss = 0.0\n",
        "      val_loss = 0.0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "\n",
        "      # Training\n",
        "      model.train()\n",
        "      for sequences, targets in train:\n",
        "          sequences, targets = sequences.to(device), targets.to(device)  # Move data to GPU\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(sequences)\n",
        "          loss = loss_fn(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss += loss.item() * sequences.size(0)\n",
        "\n",
        "      # Validation\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          for sequences, targets in test:\n",
        "              sequences, targets = sequences.to(device), targets.to(device)  # Move data to GPU\n",
        "              outputs = model(sequences)\n",
        "              loss = loss_fn(outputs, targets)\n",
        "              val_loss += loss.item() * sequences.size(0)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += targets.size(0)\n",
        "              correct += (predicted == targets).sum().item()\n",
        "\n",
        "      train_loss = train_loss / len(train.dataset)\n",
        "      val_loss = val_loss / len(test.dataset)\n",
        "      accuracy = correct / total * 100\n",
        "\n",
        "      train_loss_list.append(train_loss)\n",
        "      val_loss_list.append(val_loss)\n",
        "      val_accuracy_list.append(accuracy)\n",
        "\n",
        "      print(f'Epoch [{epoch + 1}/{epochs}], '\n",
        "            f'Training Loss: {train_loss:.4f}, '\n",
        "            f'Validation Loss: {val_loss:.4f}, '\n",
        "            f'Validation Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Define the Transformer model with dropout\n",
        "class CharTransformer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, nhead, dropout=0.1):\n",
        "        super(CharTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(hidden_size, nhead, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        transformer_output = self.transformer_encoder(embedded)\n",
        "        output = self.fc(transformer_output[:, -1, :])  # Get the output of the last Transformer block\n",
        "        return output\n",
        "\n",
        "# Prediction function\n",
        "def predict_next_char(model, sequence_length, char_to_int, int_to_char, test_str):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Convert the test string to integers\n",
        "        test_sequence = [char_to_int[char] for char in test_str]\n",
        "        test_sequence = torch.tensor(test_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Predict the next character\n",
        "        output = model(test_sequence)\n",
        "        _, predicted_index = torch.max(output, 1)\n",
        "        predicted_char = int_to_char[predicted_index.item()]\n",
        "\n",
        "    return predicted_char\n",
        "\n",
        "# Train and validate for sequence lengths of 10, 20, and 30 for both Transformer and RNN models\n",
        "results = {}\n",
        "hidden_size = 128\n",
        "num_layers = 3\n",
        "nhead = 2\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "model = CharTransformer(len(chars), hidden_size, len(chars), num_layers, nhead, dropout=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train and validate Transformer model with dropout\n",
        "training_loop(\n",
        "    train = train_loader,\n",
        "    test = test_loader,\n",
        "    model = model,\n",
        "    loss_fn = criterion,\n",
        "    optimizer = optimizer,\n",
        "    epochs = epochs\n",
        ")\n",
        "\n",
        "# Predicting the next character\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next char\"\n",
        "predicted_char = predict_next_char(model, 20, char_to_int, int_to_char, test_str)\n",
        "print(f\"LSTM: Predicted next character: '{predicted_char}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBUIYhbjq_aC",
        "outputId": "b582e8c7-23bd-4c38-a97c-a4acf74d87e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Training Loss: 2.5165, Validation Loss: 2.4828, Validation Accuracy: 26.96%\n",
            "Epoch [2/10], Training Loss: 2.4877, Validation Loss: 2.4759, Validation Accuracy: 26.75%\n",
            "Epoch [3/10], Training Loss: 2.4793, Validation Loss: 2.4692, Validation Accuracy: 26.68%\n",
            "Epoch [4/10], Training Loss: 2.4766, Validation Loss: 2.4688, Validation Accuracy: 26.93%\n",
            "Epoch [5/10], Training Loss: 2.4755, Validation Loss: 2.4677, Validation Accuracy: 26.62%\n",
            "Epoch [6/10], Training Loss: 2.4736, Validation Loss: 2.4659, Validation Accuracy: 26.96%\n",
            "Epoch [7/10], Training Loss: 2.4711, Validation Loss: 2.4665, Validation Accuracy: 26.94%\n",
            "Epoch [8/10], Training Loss: 2.4706, Validation Loss: 2.4639, Validation Accuracy: 27.01%\n",
            "Epoch [9/10], Training Loss: 2.4737, Validation Loss: 2.4635, Validation Accuracy: 26.99%\n",
            "Epoch [10/10], Training Loss: 2.4687, Validation Loss: 2.4623, Validation Accuracy: 26.90%\n",
            "LSTM: Predicted next character: 'e'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOM-Sw2tsfOn"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}